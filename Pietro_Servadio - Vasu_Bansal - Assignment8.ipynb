{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pietro Servadio, Vasu Bansal\n",
        "\n",
        "Answers at the bottom"
      ],
      "metadata": {
        "id": "QF-paPPOyW-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "10pyUMFkGKVQ"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XkJ5299Tek6B"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bsl7jBzV6_KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f90202d-66ba-4e4b-c357-900afbaaa14f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UdYv1HJUQ8XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b596dfbd-6d65-4c00-e889-734a09cb4f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
          ]
        }
      ],
      "source": [
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o9ULAJYtEvKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2ba27b-e5ea-4d26-bc98-a31b62d04cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CsB3-9uQQYyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429407c1-3ff3-409f-f188-028d681d34f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "USAJxW4RD7pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2462b5-e7c1-4ca2-a86d-0c712fad5fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SCnqEukIE9pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b871b1-829e-4827-fbe6-6030270a9efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 7): (hot, sun)\n",
            "(4, 5): (shimmered, in)\n",
            "(4, 2): (shimmered, wide)\n",
            "(1, 5): (the, in)\n",
            "(5, 6): (in, hot)\n"
          ]
        }
      ],
      "source": [
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_skip_grams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFaZsyfG6DYS",
        "outputId": "2e2abb28-0ebd-4393-fcba-d166663e175a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6, 7],\n",
              " [4, 5],\n",
              " [4, 2],\n",
              " [1, 5],\n",
              " [5, 6],\n",
              " [3, 2],\n",
              " [3, 5],\n",
              " [4, 1],\n",
              " [1, 2],\n",
              " [3, 1],\n",
              " [1, 6],\n",
              " [1, 4],\n",
              " [5, 4],\n",
              " [6, 1],\n",
              " [2, 1],\n",
              " [1, 3],\n",
              " [5, 3],\n",
              " [4, 3],\n",
              " [1, 7],\n",
              " [7, 1],\n",
              " [3, 4],\n",
              " [2, 4],\n",
              " [2, 3],\n",
              " [6, 5],\n",
              " [5, 1],\n",
              " [7, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_word, context_word = positive_skip_grams[0]\n",
        "print(target_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1FGPdP65rH2",
        "outputId": "0a00d725-f3d4-4b70-b804-c41cc89dbb4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m_LmdzqIGr5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a417af6c-254e-4230-efc6-707f6bccf35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['wide', 'the', 'shimmered', 'road']\n"
          ]
        }
      ],
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or_2QEm7tEZJ",
        "outputId": "5ab8c6b4-f068-4cc9-e16c-46bbf9df3912"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[7]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zSiZwifuLvHf"
      },
      "outputs": [],
      "source": [
        "# Reduce a dimension so you can use concatenation (in the next step).\n",
        "squeezed_context_class = tf.squeeze(context_class, 1)\n",
        "\n",
        "# Concatenate a positive context word with negative sampled words.\n",
        "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = target_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tzyCPCuZwmdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1c83ce-5a5e-442c-8670-d80d60dc920b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_index    : 6\n",
            "target_word     : hot\n",
            "context_indices : [7 2 1 4 3]\n",
            "context_words   : ['sun', 'wide', 'the', 'shimmered', 'road']\n",
            "label           : [1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x-FwkR8jx9-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f850eb16-e271-43ba-d1fd-f8e67e0f621f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target  : 6\n",
            "context : tf.Tensor([7 2 1 4 3], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "print(\"target  :\", target)\n",
        "print(\"context :\", context)\n",
        "print(\"label   :\", label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Rn9zAnDccyRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c39a4a1-138d-458f-989b-9977c972f3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
          ]
        }
      ],
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
        "print(sampling_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "63INISDEX1Hu"
      },
      "outputs": [],
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for `vocab_size` tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in the dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with a positive context word and negative samples.\n",
        "    couples = positive_skip_grams\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "\n",
        "      couples2 = couples\n",
        "      context_words = [target_word]\n",
        "      for tw, cw in couples2:     # This cycle analyses all the context words in the positive skipgram array relative to a target word\n",
        "        if tw == target_word:     # and saves them in the array \"context_words\". Also saves the target word itself.\n",
        "          context_words.append(cw)\n",
        "      #print(context_words)\n",
        "\n",
        "      while True:                 #\n",
        "\n",
        "        context_class = tf.expand_dims(\n",
        "            tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "            true_classes=context_class,\n",
        "            num_true=1,\n",
        "            num_sampled=num_ns,\n",
        "            unique=True,\n",
        "            range_max=vocab_size,\n",
        "            seed=seed,\n",
        "            name=\"negative_sampling\")\n",
        "\n",
        "        ns_candidates_numpy = negative_sampling_candidates.numpy()\n",
        "        matching = False\n",
        "        for ns in ns_candidates_numpy:    # This double cycle checks is there are matches between the positive skipgrams and the\n",
        "          for cws in context_words:       # negative skipgrams. If there are not matches, meaning that the negative skipgrams do not\n",
        "            if ns == cws:                 # contain the positive ones (nor the target word), the program moves on with\n",
        "              matching = True             # the next pair (target_word, context_word)\n",
        "        if not matching:\n",
        "          break\n",
        "        #print(negative_sampling_candidates.numpy())\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels\n",
        "\n",
        "  # To sum up the code just generates negative skipgrams until an instance of them does not contain positive skipgrams.\n",
        "  # This method we applied is obviously based on probability and could take many iteration to generate the desired skipgrams,\n",
        "  # however we only need to generate four negative skipgrams and the vocabulary size is 4096, so the odds are not terrible.\n",
        "  # Furthemore we didn't need to drastically modify the code and not even to calculate the softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QFkitxzVVaAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6279a6eb-0fc1-4f8f-f715-fc73d8763ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lfgnsUw3ofMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14db704d-72fa-4742-8c11-c8617522580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n"
          ]
        }
      ],
      "source": [
        "with open(path_to_file) as f:\n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ViDrwy-HjAs9"
      },
      "outputs": [],
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2MlsXzo-ZlfK"
      },
      "outputs": [],
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and the number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
        "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
        "# same length.\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "seZau_iYMPFT"
      },
      "outputs": [],
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jgw9pTA7MRaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66e20a8-459c-4a7b-856a-11dfc1c61bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ]
        }
      ],
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yUVYrDp0araQ"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sGXoOh9y11pM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c27910-42ea-401c-d4d5-3f90a0415dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32777\n"
          ]
        }
      ],
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I_NcvnfG3nA",
        "outputId": "42a19d71-1430-4066-8e9b-8915c89b3a02"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0]), array([138,  36, 982, 144, 673, 125,  16, 106,   0,   0]), array([34,  0,  0,  0,  0,  0,  0,  0,  0,  0]), array([106, 106,   0,   0,   0,   0,   0,   0,   0,   0]), array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0]), array([   7,   41,   34, 1286,  344,    4,  200,   64,    4, 3690]), array([34,  0,  0,  0,  0,  0,  0,  0,  0,  0]), array([1286, 1286,    0,    0,    0,    0,    0,    0,    0,    0]), array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0]), array([  89,    7,   93, 1187,  225,   12, 2442,  592,    4,    2])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WZf1RIbB2Dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572b6bb0-84e5-4433-ee33-cdfd96a24635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[   7   41   34 1286  344    4  200   64    4 3690] => ['you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[1286 1286    0    0    0    0    0    0    0    0] => ['resolved', 'resolved', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[  89    7   93 1187  225   12 2442  592    4    2] => ['first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences[:11]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "44DJ22M6nX5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdf9b5e-417f-4050-e9ee-25376333b0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32777/32777 [00:25<00:00, 1295.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (64577,)\n",
            "contexts.shape: (64577, 5)\n",
            "labels.shape: (64577, 5)\n"
          ]
        }
      ],
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in targets:\n",
        "  if i == 0:\n",
        "    print(\"yay!\")"
      ],
      "metadata": {
        "id": "6UqF63oRTdjm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nbu8PxPSnVY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0937235e-e983-4dd7-e8c8-089c0e9ef424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y5Ueg6bcFPVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46108a96-9291-4b3e-f1c5-2e248b09af0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "i9ec-sS6xd8Z"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ekQg_KbWnnmQ"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9d-ftBCeEZIR"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gmC1BJalEZIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af3e74d-9964-42cf-92bb-ee22a7a35cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 21ms/step - loss: 1.6081 - accuracy: 0.2379\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.5872 - accuracy: 0.5683\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 1.5335 - accuracy: 0.6106\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 1.4403 - accuracy: 0.5825\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 1.3323 - accuracy: 0.5928\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.2287 - accuracy: 0.6228\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 1.1339 - accuracy: 0.6590\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 2s 24ms/step - loss: 1.0474 - accuracy: 0.6934\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.9679 - accuracy: 0.7250\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.8946 - accuracy: 0.7530\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.8270 - accuracy: 0.7787\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.7648 - accuracy: 0.8010\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.7076 - accuracy: 0.8204\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.6552 - accuracy: 0.8372\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.6073 - accuracy: 0.8522\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.5637 - accuracy: 0.8653\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.5239 - accuracy: 0.8763\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.4878 - accuracy: 0.8867\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.4549 - accuracy: 0.8961\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.4250 - accuracy: 0.9043\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7933ec244040>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers\n",
        "\n",
        "1)\t14. 2 for the first and last words, 3 for the second and the second last words and 4 for the middle word.\n",
        "\n",
        "2)\tIn general, practice shows that for a big dataset it is best to keep the skip-gram window small, around 2 words each side. This implies a smaller number of skip-grams per word. Instead with small datasets it suits best a bigger window of up to 20 words per side, hence having more skip-grams per word.\n",
        "\n",
        "3)\tIn the denominator of the softmax needs to be computed over the entire vocabulary which is computationally heavy.\n",
        "\n",
        "4)\tThey are different each time because the log_uniform_candidate_sampler() function choses the samples randomly.\n",
        "\n",
        "5)\tNo, the max length of a sentence is limited to 10 words, hence it is not even granted that a word within a sentence is used in the training dataset. Every line is treated separately to other lines not admitting “cross-sentence context”.\n",
        "\n",
        "6)\tThe index zero (aka padding) is found in context vectors but not in the targets. This makes sense because a word can be typically used at the end of a sentence (or in the middle of it), hence more frequently near padding (or away from it).\n",
        "\n",
        "7)The sampling table is a list of probabilities based word-frequency ranks. The function assumes a Zipf's distribution of the word frequencies for sampling, I.E sampling_table[i] denotes the probability of sampling the i-th most common word in dataset and therefore we do not need to make a reference to actual text data."
      ],
      "metadata": {
        "id": "ae-3tqcpx8bK"
      }
    }
  ]
}